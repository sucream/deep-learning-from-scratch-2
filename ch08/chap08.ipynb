{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# softmax forward, backward\n",
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.out = Softmax.softmax(x)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = self.out * dout\n",
    "        sumdx = np.sum(dx, axis=1, keepdims=True)\n",
    "        dx -= self.out * sumdx\n",
    "        return dx\n",
    "    \n",
    "    # softmax 연산\n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        if x.ndim == 2:\n",
    "            x = x - x.max(axis=1, keepdims=True)\n",
    "            x = np.exp(x)\n",
    "            x /= x.sum(axis=1, keepdims=True)\n",
    "        elif x.ndim == 1:\n",
    "            x = x - np.max(x)\n",
    "            x = np.exp(x) / np.sum(np.exp(x))\n",
    "    \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 디코더의 현재 히든 상태값과 인코더의 각 상태별 히든값의 내적 및 정규화\n",
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "\n",
    "    \n",
    "    def forward(self, hs, h):\n",
    "        \"\"\"\n",
    "        인코더의 각 상태값과 디코더의 현재 히든값을 내적 및 정규화\n",
    "        결국 현재 디코더의 히든 스테이트 기준, 인코더의 모든 단어들의 히든 스테이트의 유사도 측정\n",
    "        :return: \n",
    "        :param hs: 인코더의 전체 히든 스테이트\n",
    "        :param h: 디코더의 현재 히든 스테이트\n",
    "        :return: 내적하고 정규화한거\n",
    "        \"\"\"\n",
    "        num_sample, time_stap, hidden_size = hs.shape\n",
    "\n",
    "        # h의 원래 shape은 (num_sample, hidden_size)인데 hs랑 내적해야해서 같은 형태로 만들어줌\n",
    "        hr = h.reshape(num_sample, 1, hidden_size)#.repeat(T, axis=1)\n",
    "        t = hs * hr  # 같은 shape의 numpy array를 곱하면 원소별 곱\n",
    "        s = np.sum(t, axis=2)  # 원소별 덧셈 = 내적한 결과\n",
    "        a = self.softmax.forward(s)  # 정규화를 위한 softmax\n",
    "\n",
    "        self.cache = (hs, hr)  # 역전파를 위해 저장☆\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        \"\"\"\n",
    "        상위로부터 받은 da로 역전파\n",
    "        sofrmax -> sum -> 원소곱 순으로 역전파 진행\n",
    "        :param da: 상위 계층으로부터 전달받은 a의 오차값\n",
    "        :return: hs와 h의 오차\n",
    "        \"\"\"\n",
    "        hs, hr = self.cache\n",
    "        num_sample, time_stap, hidden_size = hs.shape\n",
    "\n",
    "        ds = self.softmax.backward(da)  # softmax 역전파하면 ds\n",
    "        dt = ds.reshape(num_sample, time_stap, 1).repeat(hidden_size, axis=2)  # 덧셈노드의 역전파는 repeat노드~\n",
    "        # t가 hs * hr이니까 각각 반대로 곱해주자 -> 곱셈노드는 반대로 곱해주기~\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)  # repeat노드였으니까 sum노드~\n",
    "\n",
    "        return dhs, dh\n",
    "   "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## AttentionWeight 테스트\n",
    " "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.11195273, 0.16978945, 0.09138643, 0.36925081, 0.2576206 ],\n       [0.20106812, 0.1687147 , 0.1702468 , 0.14653598, 0.3134344 ],\n       [0.27515615, 0.21627004, 0.39823815, 0.01212188, 0.09821378],\n       [0.11936457, 0.0697606 , 0.07891963, 0.41000414, 0.32195106],\n       [0.01158808, 0.56577483, 0.29054582, 0.06214907, 0.0699422 ],\n       [0.35247731, 0.05567077, 0.2022596 , 0.27362803, 0.11596429],\n       [0.00071705, 0.47902672, 0.44651269, 0.07109236, 0.00265119],\n       [0.05590267, 0.18602931, 0.47905771, 0.01412347, 0.26488684],\n       [0.27253952, 0.29879283, 0.07418869, 0.16821585, 0.18626311],\n       [0.09951457, 0.38910379, 0.0379079 , 0.01428984, 0.4591839 ]])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 4
    }
   ],
   "source": [
    "num_sample = 10\n",
    "time_stap = 5\n",
    "hidden_size = 4\n",
    "\n",
    "hs = np.random.randn(num_sample, time_stap, hidden_size)\n",
    "hs\n",
    "\n",
    "h = np.random.randn(num_sample, hidden_size)\n",
    "h\n",
    "\n",
    "hr = h.reshape(num_sample, 1, hidden_size).repeat(time_stap, axis=1)\n",
    "hr\n",
    "\n",
    "t = hs * hr\n",
    "t\n",
    "\n",
    "s = np.sum(t, axis=2)\n",
    "s\n",
    "\n",
    "softmax = Softmax()\n",
    "a = softmax.forward(s)\n",
    "a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, a):\n",
    "        \"\"\"\n",
    "        인코더의 모든 히든 스테이트 hs와 각 단어들의 가중합\n",
    "        :param hs: 인코더의 모든 히든 스테이트\n",
    "        :param a: 각 단어별 가중치\n",
    "        :return: 가중합된 맥락벡터\n",
    "        \"\"\"\n",
    "        num_sample, time_stap, hidden_size = hs.shape\n",
    "\n",
    "        # 위랑 거의 같음\n",
    "        ar = a.reshape(num_sample, time_stap, 1)#.repeat(T, axis=1)\n",
    "        t = hs * ar  # 가중치를 곱해요\n",
    "        c = np.sum(t, axis=1)  # 만들어진 벡터들을 더하여 맥락벡터 만듬, 근데 전체 더하는게 전체 맥락에 도움을 주나? 각 벡터를 따로 쓰는 방법 있으면 좋을듯\n",
    "\n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "\n",
    "    def backward(self, dc):\n",
    "        \"\"\"\n",
    "        역전파\n",
    "        sum -> 곱셈 -> repeat 순으로 역전파\n",
    "        :param dc: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        hs, ar = self.cache\n",
    "        num_sample, time_stap, hidden_size = hs.shape\n",
    "        dt = dc.reshape(num_sample, 1, hidden_size).repeat(time_stap, axis=1)  # sum노드의 역전파는 repeat노드~\n",
    "        # t가 hs * ar이니까 각각 반대로 곱해주자 -> 곱셈노드는 반대로 곱해주기~\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)  # repeat노드의 역전파는 sum노드~\n",
    "\n",
    "        return dhs, da"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-0.09466044,  0.71296525,  1.22616427, -0.65008742])"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 6
    }
   ],
   "source": [
    "# 샘플 고려 안하고 한개로 가정\n",
    "time_stap = 5\n",
    "hidden_size = 4\n",
    "\n",
    "hs = np.random.randn(time_stap, hidden_size)\n",
    "hs\n",
    "\n",
    "a = np.array([0.8, 0.1, 0.03, 0.05, 0.02])\n",
    "a\n",
    "\n",
    "ar = a.reshape(time_stap, 1).repeat(hidden_size, axis=1)\n",
    "ar\n",
    "\n",
    "t = hs * ar\n",
    "t\n",
    "\n",
    "c = np.sum(t, axis=0)\n",
    "c"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Attention 클래스"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# 해당 클래스는 한 타임만 처리하는 클래스임을 잊지 말자\n",
    "# 이후 TimeAttention 클래스에서 모든 타임에 대해 처리해줌\n",
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()  # 어텐션 웨이트를 구해주는 친구\n",
    "        self.weight_sum_layer = WeightSum()  # 구해진 어텐션 웨이트랑 웨이트섬 하는 친구\n",
    "        self.attention_weight = None  # 시각화를 위해 어텐션 웨이트 저장☆\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)  # 인코더의 hs와 현재 타임의 디코더 h값으로 어텐션 가중치 획득\n",
    "        out = self.weight_sum_layer.forward(hs, a)  # 다시 인코더의 hs와 어텐션 가중치로 웨이트섬~\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # 당연히 반대로 weight_sum -> attention_weight\n",
    "        # hs의 경우, 두 연산에 모두 사용되서 두 연산의 오차를 받아 더한 값을 최종 dhs로 사용\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Attention 클래스"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 전체 타임에 대해서 처리해주는 어텐션 클래스\n",
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "\n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        \"\"\"\n",
    "        타임별 어텐션 처리 해줌\n",
    "        :param hs_enc: 인코더의 모든 히든 스테이트\n",
    "        :param hs_dec: 디코더의 모든 히든 스테이트\n",
    "        :return: \n",
    "        \"\"\"\n",
    "        num_sample, time_stap, hidden_size = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "\n",
    "        for t in range(time_stap):  # 가지고 있는 타임 스탭만큼 반복\n",
    "            layer = Attention()  # 위에서 본 어텐션을 해당 타임을 처리하기 위해 하나 만듬\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])  # 인코더의 모든 히든 스테이트와 해당 타임의 디코더의 히든 스테이트를 넘겨 어텐션 결과 저장\n",
    "            self.layers.append(layer)  # 레이어 추가\n",
    "            self.attention_weights.append(layer.attention_weight)  # 시각화에 쓰려고 저장\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        num_sample, time_stap, hidden_size = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "\n",
    "        for t in range(time_stap):  # 타임 스탭만큼 반복, 어텐션 계층은 hs와 lstm의 결과로 연산이 이루어져서 순서대로 역전파해도 각 어텐션층이 독립적?임\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])  # 각 타임별 역전파\n",
    "            dhs_enc += dhs  # 인코더쪽 최종 dh는 모든 타임의 dhs의 합\n",
    "            dhs_dec[:,t,:] = dh  # 디코더의 각 타임별 dh\n",
    "\n",
    "        return dhs_enc, dhs_dec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}