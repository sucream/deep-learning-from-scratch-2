{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "# Chap 4 : Word2Vec 속도 개선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다음 두 가지를 진행\n",
    "- 입력층 원핫 벡터와 가중치 $W_{in}$ 간의 곱 계산에서 발생하는 문제 => Embedding 계층 도입\n",
    "- 은닉층과 가중치 $W_{out}$ 간의 곱, Softmax 계층 계산에서 발생하는 문제 => Negative Sampling 도입\n",
    "\n",
    "![ㅁㄴㅇㄹ](img/chap_04/01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Em|bedding 계층\n",
    "- 단어를 원핫으로의 변환과 MatMul 계층은 사실상 $W_{in}$의 특정 행을 추출하는 작업\n",
    "- 단어 ID에 해당하는 행을 추출하는 계층을 통해 해결\n",
    "\n",
    "![ㅁㄴㅇㄹ](img/chap_04/02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Embedding 계층 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 3,  4,  5],\n",
       "       [ 6,  7,  8],\n",
       "       [ 9, 10, 11],\n",
       "       [12, 13, 14],\n",
       "       [15, 16, 17],\n",
       "       [18, 19, 20]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "W = np.arange(21).reshape(7,3)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 7, 8])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 미니배치를 고려한 행 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2],\n",
       "       [ 3,  4,  5],\n",
       "       [ 9, 10, 11]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[[0, 1, 3]]\n",
    "# idx = np.array([0, 1, 3])\n",
    "# W[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Embedding 계층의 forward() 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]  # 하나밖에 없지만 레이어 생성 규칙이라 배열\n",
    "        self.grads = [np.zeros_like(W)]  # 하나밖에 없지만 레이어 생성 규칙이라 배열\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.65039271,  0.69025171,  0.52157334, -0.30134532],\n",
       "       [-1.44200684,  0.82186024,  1.15116599,  1.25567187],\n",
       "       [ 1.17411856,  0.34702447, -0.61113509, -0.04681875],\n",
       "       [ 2.82794695,  1.73938041,  0.5528964 ,  0.45970765]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = np.random.randn(4, 4)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.65039271,  0.69025171,  0.52157334, -0.30134532],\n",
       "       [ 1.17411856,  0.34702447, -0.61113509, -0.04681875]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embdding = Embedding(W)\n",
    "\n",
    "embdding.forward([0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. Embedding 계층의 backward() 구현\n",
    "- 앞 층으로부터 전해진 기울기를 가중치 기울기 $dW$의 특정 행에 설정\n",
    "\n",
    "![ㅁㄴㅇㄹ](img/chap_04/03.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]  # 하나밖에 없지만 레이어 생성 규칙이라 배열\n",
    "        self.grads = [np.zeros_like(W)]  # 하나밖에 없지만 레이어 생성 규칙이라 배열\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0  # 기울기의 형상을 유지한 상태로 0으로 초기화\n",
    "        dW[self.idx] = dout\n",
    "        print(\"id(self.grads):\", id(self.grads))\n",
    "        print(\"type(self.grads):\", type(self.grads))\n",
    "        print(self.grads)\n",
    "        print()\n",
    "        print(\"id(dW):\", id(dW))\n",
    "        print(\"type(dW):\", type(dW))\n",
    "        print(dW)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(self.grads): 2575983732744\n",
      "type(self.grads): <class 'list'>\n",
      "[array([[ 0.21653844, -0.79088163,  0.10271234,  0.35839855],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "       [ 0.43173284, -1.36803628,  0.80290825,  0.11675985],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ]])]\n",
      "\n",
      "id(dW): 2575983858912\n",
      "type(dW): <class 'numpy.ndarray'>\n",
      "[[ 0.21653844 -0.79088163  0.10271234  0.35839855]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.43173284 -1.36803628  0.80290825  0.11675985]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randn(4, 4)\n",
    "W\n",
    "embdding = Embedding(W)\n",
    "\n",
    "out = embdding.forward([0, 2])\n",
    "embdding.backward(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4. Backward() 구현의 문제 및 해결\n",
    "- idx의 원소가 중복될 경우, $dW$를 덮어쓰는 문제 발생\n",
    "- 이를 해결하기 위해 해당 행의 $dW$가 존재한다면, 값을 더함(Repeat 노드의 역전파)\n",
    "\n",
    "![ㅁㄴㅇㄹ](img/chap_04/04.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, W):\n",
    "        self.params = [W]  # 하나밖에 없지만 레이어 생성 규칙이라 배열\n",
    "        self.grads = [np.zeros_like(W)]  # 하나밖에 없지만 레이어 생성 규칙이라 배열\n",
    "        \n",
    "    def forward(self, idx):\n",
    "        W, = self.params\n",
    "        self.idx = idx\n",
    "        out = W[idx]\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dW, = self.grads\n",
    "        dW[...] = 0  # 기울기의 형상을 유지한 상태로 0으로 초기화\n",
    "        \n",
    "        for i, word_id in enumerate(self.idx):\n",
    "            dW[word_id] += dout[i]  # Repeat 노드의 역전파 = Sum 노드\n",
    "            \n",
    "        # np.add.at(dW, self.idx, dout)\n",
    "        \n",
    "        print(\"id(self.grads):\", id(self.grads))\n",
    "        print(\"type(self.grads):\", type(self.grads))\n",
    "        print(self.grads)\n",
    "        print()\n",
    "        print(\"id(dW):\", id(dW))\n",
    "        print(\"type(dW):\", type(dW))\n",
    "        print(dW)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(self.grads): 2575983875208\n",
      "type(self.grads): <class 'list'>\n",
      "[array([[ 1.59015297, -1.50133133,  0.70998179,  0.09865617],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
      "       [ 1.17473646, -1.54453985, -1.65016348, -0.49054014],\n",
      "       [ 0.        ,  0.        ,  0.        ,  0.        ]])]\n",
      "\n",
      "id(dW): 2575983905088\n",
      "type(dW): <class 'numpy.ndarray'>\n",
      "[[ 1.59015297 -1.50133133  0.70998179  0.09865617]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 1.17473646 -1.54453985 -1.65016348 -0.49054014]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randn(4, 4)\n",
    "W\n",
    "embdding = Embedding(W)\n",
    "\n",
    "out = embdding.forward([0, 2])\n",
    "embdding.backward(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Nagative sampling\n",
    "- Embedding 적용후 은닉층 이후 계산이 오래 걸림\n",
    "    - 은닉층 뉴런과 가중치 행렬 $W_{out}$의 곱\n",
    "    - Softmax 계층 계산\n",
    "- 다중 분류 => 이진분류로 근사\n",
    "\n",
    "![ㅁㄴㅇㄹ](img/chap_04/05.png)\n",
    "![ㅁㄴㅇㄹ](img/chap_04/06.png)\n",
    "![ㅁㄴㅇㄹ](img/chap_04/07.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Sigmoid with Loss\n",
    "- 이진 분류 문제로 변환하기 때문에 sigmoid와 Cross Entropy Error 사용\n",
    "\n",
    "![image.png](img/chap_04/08.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. 이진분류 구현\n",
    "- 기존 다중분류를 수행하는 모델\n",
    "![image.png](img/chap_04/09.png)\n",
    "\n",
    "- 이진분류를 수행하는 모델\n",
    "![image.png](img/chap_04/10.png)\n",
    "\n",
    "- 은닉층 이후를 Embedding 계층과 dot 연산을 합친 Embedding Dot 계층을 이용\n",
    "![image.png](img/chap_04/11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EmbeddingDot 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDot:\n",
    "    def __init__(self, W):\n",
    "        self.embed = Embedding(W)  # 기존 임베딩 계층과 동일\n",
    "        self.params = self.embed.params\n",
    "        self.grads = self.embed.grads\n",
    "        self.cache = None  # 순전파 계산 결과 임시 저장용\n",
    "        \n",
    "    def forward(self, h, idx):\n",
    "        target_W = self.embed.forward(idx)  # idx는 미니배치를 위해 리스트\n",
    "        out = np.sum(target_W * h, axis=1)  # 각 벡터의 내적을 쉽게 구하는 트릭\n",
    "        self.cache = (h, target_W)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        h, target_W = self.cache\n",
    "        dout = dout.reshape(dout.shape[0], 1)\n",
    "        \n",
    "        dtarget_W = dout * h\n",
    "        self.embed.backward(dtarget_W)\n",
    "        dh = dout * target_W\n",
    "        return dh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SigmoidWithLoss 클래스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cross Entropy Error\n",
    "\n",
    "$L = -(t\\log{y} + (1 - t)\\log(1-y))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 정답 데이터가 원핫 벡터일 경우 정답 레이블 인덱스로 변환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "class SigmoidWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.loss = None\n",
    "        self.y = None  # sigmoid의 출력\n",
    "        self.t = None  # 정답 데이터\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = 1 / (1 + np.exp(-x))  # 시그모이드\n",
    "\n",
    "        self.loss = cross_entropy_error(np.c_[1 - self.y, self.y], self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "\n",
    "        dx = (self.y - self.t) * dout / batch_size\n",
    "        return dx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. Negative sampling\n",
    "- 다중 분류 문제를 이진 분류 문제로 변환하였지만, 긍정적인 예만 학습\n",
    "- 부정적인 예도 학습하여 모델의 분류 능력을 향상시킬 필요 있음\n",
    "- 모든 부정적 예를 다 학습하는 것은 힘들고 이진 분류를 하는 이점이 없음\n",
    "- 적은 수의 부정적인 예를 샘플링하여 같이 학습\n",
    "\n",
    "![image.png](img/chap_04/12.png)\n",
    "![image.png](img/chap_04/13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 샘플링 기법\n",
    "- 말뭉치에서 각 단어의 빈도수를 구하여 확률 분포로 나타냄\n",
    "- 말뭉치에서 자주 등장하는 단어를 많이 추출\n",
    "- 드물게 등장하는 단어를 적게 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "9\n",
      "1\n",
      "8\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(np.random.choice(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'say'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['you', 'say', 'goodbye', 'I', 'hello', '.']\n",
    "np.random.choice(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['say', 'say', 'goodbye'], dtype='<U7')"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(words, size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['.', 'say', 'hello'], dtype='<U7')"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(words, size=3, replace=False)  # 중복 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.6939333 , -0.56233325,  0.68469457,  0.95888722,  0.63085552,\n",
       "       -0.20714661])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.random.randn(6)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "    elif x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05980768, 0.06821975, 0.23740367, 0.31229698, 0.22496007,\n",
       "       0.09731185])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = softmax(data)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you'"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(words, p=p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2vec 네거티브 샘플링은 확률분포에 0.75를 곱할 것을 권고\n",
    "- 출현이 낮은 단어를 버리지 않기 위해 사용\n",
    "- 확률이 낮은 단어의 확률을 좀 높여, 샘플링될 확률을 올려줌\n",
    "\n",
    "$P'(w_i) = \\frac{P(w_i)^{0.75}}{\\sum_{j}^{n}{P(w_j)^{0.75}}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.05980767962382291, 0.07992460237183979),\n",
       " (0.06821975127705387, 0.08821559507997324),\n",
       " (0.2374036723322005, 0.2247648236380674),\n",
       " (0.3122969820199227, 0.2760823289193763),\n",
       " (0.22496006518151723, 0.2158697761778994),\n",
       " (0.09731184956548286, 0.11514287381284394)]"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_p = np.power(p, 0.75)\n",
    "new_p /= np.sum(new_p)\n",
    "\n",
    "[(i, j) for i, j in zip(p, new_p)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4 부정적 예를 샘플링하는 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class UnigramSampler:\n",
    "    def __init__(self, corpus, power, sample_size):\n",
    "        self.sample_size = sample_size\n",
    "        self.vocab_size = None\n",
    "        self.word_p = None\n",
    "\n",
    "        counts = collections.Counter()\n",
    "        for word_id in corpus:\n",
    "            counts[word_id] += 1\n",
    "\n",
    "        vocab_size = len(counts)\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.word_p = np.zeros(vocab_size)\n",
    "        for i in range(vocab_size):\n",
    "            self.word_p[i] = counts[i]\n",
    "\n",
    "        self.word_p = np.power(self.word_p, power)\n",
    "        self.word_p /= np.sum(self.word_p)\n",
    "\n",
    "    def get_negative_sample(self, target):\n",
    "        batch_size = target.shape[0]\n",
    "\n",
    "        GPU = False\n",
    "        if not GPU:\n",
    "            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                p = self.word_p.copy()\n",
    "                target_idx = target[i]\n",
    "                p[target_idx] = 0\n",
    "                p /= p.sum()\n",
    "                negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)\n",
    "        else:\n",
    "            # GPU(cupy）로 계산할 때는 속도를 우선한다.\n",
    "            # 부정적 예에 타깃이 포함될 수 있다.\n",
    "            negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),\n",
    "                                               replace=True, p=self.word_p)\n",
    "\n",
    "        return negative_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 2],\n",
       "       [1, 0],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = np.array([0, 1, 2, 3, 4, 1, 2, 3])\n",
    "power = 0.75\n",
    "sample_size = 2\n",
    "\n",
    "sampler = UnigramSampler(corpus, power, sample_size)\n",
    "target = np.array([1, 3, 0])\n",
    "negative_sample = sampler.get_negative_sample(target)\n",
    "negative_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-5 NegativeSampling 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
